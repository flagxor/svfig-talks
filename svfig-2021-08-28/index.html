<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Deep Learning in Forth - Part I</title>
    <meta name="description"
          content="Deep Learning in Forth - Part I - August 28, 2021">
    <meta name="author" content="Brad Nelson">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style"
    content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/league.css" id="theme">
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="../common/forth.css">
    <script src="../common/printing.js"></script>
    <script src="../node/node_modules/vis-network/standalone/umd/vis-network.min.js"></script>

<!--

Deep Learning in Forth - Part I

In recent years, multilayer artificial neural networks have made tremendous
strides solving a wide range of machine learning problems from image
recognition to the generation of random cats. While typical ML libraries
have hundreds of thousands of lines of code, Forth can be used to explore
this domain with far less complexity.

This talk will kick off a multi-part series erecting a lexicon for ML programs in Forth.

In Part I, we'll introduce key deep learning ideas including: convolutional
neural networks, automatic differentiation, and gradient descent.
We'll then build and benchmark low level building blocks for linear algebra
and format conversion that later parts in the series will use as a foundation.
-->

    <style>
pre {
  background-color: #202;
  padding: 10px;
}
.reveal-viewport {
  background: #1c0020;
  background: -moz-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -webkit-gradient(radial, center center, 0px, center center, 100%, color-stop(0%, #55005f), color-stop(100%, #1c0020));
  background: -webkit-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -o-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -ms-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background-color: #2b002b;
}
.reveal h2 {
  color: #ff4;
}
.reveal h3 {
  color: #ffc;
}
.hljs-comment {
  color: #ff0;
}
.reveal a:link {
  color: #f0f;
}
.reveal a:visited {
  color: #f0f;
}
.reveal a:hover {
  color: #fff;
}
.reveal a:active {
  color: #fff;
}
.reveal .controls {
  color: #a0a;
}
.reveal .progress {
  color: #707;
}
  </style>
  </head>

  <body>
    <div style="position: fixed; bottom: 0; width: 100%">
      <center>
        <h2 style="font-family=monospace; color: blue" id="cc1"></h2>
        <h2 style="font-family=monospace" id="cc2"></h2>
      </center>
    </div>
    <div class="reveal">

      <div class="slides">
        <section data-transition="fade-out">
          <h2>Deep Learning in Forth<br/>Part I</h2>
          <h3>August 28, 2021</h3>
          <p>
          <small><a href="http://flagxor.com">Brad Nelson</a> /
            <a href="http://twitter.com/flagxor">@flagxor</a></small>
          </p>
        </section>

        <section data-transition="fade-out">
          <h2>Topics</h2>
          <ul>
            <li>Artificial Neural Networks</li>
            <li>Convolutional Neural Networks</li>
            <li>Automatic Differentiation</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Neurons</h2>
          <ul>
            <li>Firing frequency varies by stimulus</li>
            <li>Learning mechanism poorly understood</li>
          </ul>
          <img style="background-color: white; padding: 10px"
               src="https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png"><br/>
          <small>https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Neuron3.png</small>
        </section>

        <section data-transition="fade-out">
          <h2>Artificial Neurons</h2>
          <ul>
            <li>Mathematical / Engineering construct, inspired by biology</li>
            <li>Variations in layering and activation function</li>
          </ul>
          \[ y_j = A\left(\sum_{i=1}^n w_{i,j} x_i + b_j\right) \]
          A = activation function<br/>
          w<sub>i,j</sub> = synapse weight<br/>
          b<sub>j</sub> = bias
        </section>

        <section data-transition="fade-out">
          <h2>Activation Functions</h2>
          <table>
          <tr>
            <td>\[ logistic(x) = \frac{1}{1 + e^{-x}} \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/180px-Activation_logistic.svg.png"></td>
          </tr>
          <tr>
            <td>\[ tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/180px-Activation_tanh.svg.png"></td>
          </tr>
          <tr>
            <td>\[ relu(x) = max(0, x) \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/180px-Activation_rectified_linear.svg.png"></td>
          </tr>
          </table>
        </section>

        <section data-transition="fade-out">
          <h2>Perceptron - 1958</h2>
          <ul>
            <li>Hardware Artificial Neural Network</li>
            <li>Single Layer (Linear Planes)</li>
          </ul>
          <img src="https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg"><br/>
          <small>https://en.wikipedia.org/wiki/Perceptron</small>
        </section>

        <section data-transition="fade-out">
          <h2>Discrete Convolution</h2>
          \[
            (f * g)[n] = \sum_{m=-\infty}^{\infty} f[n - m] g[m]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>2D Discrete Convolution</h2>
          \[
            (f * g)[x, y] = \sum_{i=-\infty}^{\infty}
                            \sum_{j=-\infty}^{\infty} f[x - i, y - j] g[i, j]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>2D Discrete Convolution</h2>
          <table style="background-color: white; color: black; filter: grayscale(1)">
          <tr>
            <td>Identity</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png"></td>
          </tr>
          <tr>
            <td>Edge Detection</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f800ad5f76b6c26c729ff0c1fef44284d7cade7a"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/6/6d/Vd-Edge3.png"></td>
          </tr>
          <tr>
            <td>Gaussian Blur</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ca9c0da52fe7818783942b06aac9cf396ae628bf"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/2/28/Vd-Blur1.png"></td>
          </tr>
          </table>
          <small>https://en.wikipedia.org/wiki/Kernel_(image_processing)</small>
        </section>

        <section data-transition="fade-out">
          <h2>im2col</h2>
          \[
             F * I = im2col(F, F_s) \cdot im2col(I, F_s)
          \]
        </section>

        <section data-transition="fade-out">
          \[
             I = \left[\begin{matrix}
               1^* & 2^* & 3 & 4 \\
               5^* & 6^* & 7 & 8 \\
               9 & 10 & 11 & 12 \\
               13 & 14 & 15 & 16 \\
             \end{matrix}\right]
          \]
          \[
             im2col(I, [2, 2]) = \\
             \left[\begin{matrix}
               1^* & 2 & 3 & 5 & 6 & 7 & 9 & 10 & 11 \\
               2^* & 3 & 4 & 6 & 7 & 8 & 10 & 11 & 12 \\
               5^* & 6 & 7 & 9 & 10 & 11 & 13 & 14 & 15 \\
               6^* & 7 & 8 & 10 & 11 & 12 & 14 & 15 & 16 \\
             \end{matrix}\right]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>BLAS</h2>
          <h3>Basic Linear Algebra Subprograms</h3>
          <ul>
            <li>FORTRAN Library</li>
            <li>Terse â‰¤6 character names</li>
            <li>Standard "Kernels"</li>
            <li>Strided Data</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Type Convensions</h2>
          <code>
            <b style="color: #ff0">S</b>GEMM
            <b style="color: #ff0">D</b>GEMM
            <b style="color: #ff0">C</b>GEMM
            <b style="color: #ff0">Z</b>GEMM
          </code>
          <ul style="list-style-type: none">
            <li>S = Single precision (32-bit) floats</li>
            <li>D = Double precision (64-bit) floats</li>
            <li>C = Single precision complex (2 x 32-bit) floats</li>
            <li>Z = Double precision complex (2 x 64-bit) floats</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 1 (1979)</h2>
          <ul style="list-style-type: none">
            <li>*ROTG + SROT - Givens rotation</li>
            <li>*SWAP - Swap two vectors</li>
            <li>*SCAL - Vector scaling x = a*x</li>
            <li>*COPY - Vector copy</li>
            <li><b style="color: #ffc">*AXPY - Vector scale and add y = a*x + y</b></li>
            <li><b style="color: #ffc">*DOT - Dot product</b></li>
            <li>*NRM2 - Euclidean norm / vector length</li>
            <li>*ASUM - Vector sum of absolute values</li>
            <li>I*AMAX - Index of max absolute value</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 2 (1984-88)</h2>
          <ul style="list-style-type: none">
            <li>*GEMV - Matrix Vector Multiply
                \[ y := \alpha A x + \beta y \]</li>
            <li>*GER - Row-column Vector multiply
                \[ A := \alpha x y^T + A \]</li>
            <li>Matrix-vector operations on symmetric/triangular matrices
                stored in banded or packed format.</li>
            <li>Transpose generously supported.</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 3 (1990)</h2>
          <ul style="list-style-type: none">
            <li>*GEMM - Matrix Matrix Multiply
                \[ C := \alpha A B + \beta C \]</li>
            <li>Matrix-matrix operations on symmetric/triangular matrices
                stored in banded or packed format.</li>
            <li>Transpose generously supported.</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Chain Rule</h2>
          <div>
          \[ \frac{df}{dx} = \frac{df}{du} \frac{du}{dx} \]
          </div>
        </section>

        <section data-transition="fade-out">
          <h2>Multivariable Chain Rule</h2>
          <div>
          \[ \frac{d}{dx} f(g_1(x), ..., g_k(x)) =
                 \sum_{i=1}^{k} \frac{df}{dg_i} \frac{dg_i}{dx} \]
          </div>
        </section>

        <section data-transition="fade-out">
          <h2>StyleGAN (2019)</h2>
          <img height="500" src="https://github.com/NVlabs/stylegan/raw/master/stylegan-teaser.png">
          <div class="tiny">
          https://github.com/NVlabs/stylegan/raw/master/stylegan-teaser.png</div>
        </section>

        <section data-transition="fade-out">
          <h2>Convolutional Filters</h2>
          <img height="500" src="https://developer-blogs.nvidia.com/wp-content/uploads/2014/10/caffenet_learned_filters.png">
          <div class="tiny">
          https://developer.nvidia.com/blog/deep-learning-computer-vision-caffe-cudnn/</div>
        </section>

        <section data-transition="fade-out">
          <h2>MNIST (1998)</h2>
          <ul>
            <li>Handwritten digit dataset</li>
            <li>28x28 grayscale</li>
            <li>60,000 training images</li>
            <li>10,000 test images</li>
            <li>Remixed of NIST high school student and census bureau datasets</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>MNIST (1998)</h2>
          <img height="500" src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/480px-MnistExamples.png">
          <div class="tiny">
          https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png</div>
        </section>

        <section data-transition="fade-out">
          <h2>IDX File Format</h2>
          <pre>
Big Endian

$00    - Magic #                            (byte)
$00    - Magic #                            (byte)
type   - $08=uint8 $09=int8 $0B=int16       (byte)
         $0C=int32 $0D=float32 $0E=float64
rank   - Number of dimensions               (byte)
dim 1  - Size in dimension 1                (int32)
....                                        ....
dim N  - Size in dimension N                (int32)
data   - Raw data
          </pre>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY</h2>
          <pre data-id="code-animation"><code data-trim>
       SUBROUTINE saxpy(N,SA,SX,INCX,SY,INCY)
       REAL SA
       INTEGER INCX,INCY,N
       REAL SX(*),SY(*)
       INTEGER I,IX,IY,M,MP1
       INTRINSIC mod
 
       IF (n.LE.0) RETURN
       IF (sa.EQ.0.0) RETURN
       IF (incx.EQ.1 .AND. incy.EQ.1) THEN
          m = mod(n,4)
          IF (m.NE.0) THEN
             DO i = 1,m
                sy(i) = sy(i) + sa*sx(i)
             END DO
          END IF
          IF (n.LT.4) RETURN
          mp1 = m + 1
          DO i = mp1,n,4
             sy(i) = sy(i) + sa*sx(i)
             sy(i+1) = sy(i+1) + sa*sx(i+1)
             sy(i+2) = sy(i+2) + sa*sx(i+2)
             sy(i+3) = sy(i+3) + sa*sx(i+3)
          END DO
       ELSE
          ix = 1
          iy = 1
          IF (incx.LT.0) ix = (-n+1)*incx + 1
          IF (incy.LT.0) iy = (-n+1)*incy + 1
          DO i = 1,n
           sy(iy) = sy(iy) + sa*sx(ix)
           ix = ix + incx
           iy = iy + incy
          END DO
       END IF
       RETURN
       END
					</code></pre>
          <small>http://www.netlib.org/lapack/explore-html/d8/daf/saxpy_8f_source.html</small>
        </section>

        <section data-transition="fade-out">
          <h2>Utilities</h2>
          <pre data-id="code-animation"><code data-trim>
1 sfloats constant sfloat
1 dfloats constant dfloat
variable incx   sfloat incx !
variable incy   sfloat incy !

: sf, ( r -- ) here sf! sfloat allot ;
: df, ( r -- ) here df! dfloat allot ;

: sxsy+ ( sx sy -- sx' sy' ) incy @ + swap incx @ + swap ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY</h2>
          <pre data-id="code-animation"><code data-trim>
: saxpy ( sx sy n f: sa -- )
  0 ?do over
    sf@ fover f* dup sf@ f+ dup sf! sxsy+
  loop 2drop fdrop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Basic Ops</h2>
          <pre data-id="code-animation"><code data-trim>
: sswap ( sx sy n -- )
   0 ?do dup sf@ over sf@ dup sf! over sf! sxsy+ loop 2drop ;

: sscal ( sx n f: sa -- )
  0 ?do dup sf@ fover f* dup sf! incx @ + loop drop fdrop ;

: scopy ( sx sy n -- )
  0 ?do over sf@ dup sf! sxsy+ loop 2drop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SDOT</h2>
          <pre data-id="code-animation"><code data-trim>
: sdsdot ( sx sy n f: sb -- f: prod )
  0 ?do over sf@ dup sf@ f* f+ sxsy+ loop 2drop ;
: sdot ( sx sy n -- f: prod ) 0e sdsdot ;
: dsdot ( sx sy n -- f: prod ) 0e sdsdot ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Summing and Searching</h2>
          <pre data-id="code-animation"><code data-trim>
: snrm2 ( sx n -- f: norm )
  0e 0 ?do dup sf@ fdup f* f+ incx @ + loop drop fsqrt ;

: sasum ( sx n -- f: abssum )
  0e 0 ?do dup sf@ fabs f+ incx @ + loop drop ;

: isamax ( sx n -- index )
  over sf@ fabs 0 -rot 0 ?do
    dup sf@ fabs fover f> if fdrop dup sf@ fabs nip i swap then
    incx @ +
  loop drop fdrop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Givens Rotation</h2>
          \[
             \left[\begin{matrix}
               c & -s \\
               s & c \\
             \end{matrix}\right]
             \left[\begin{matrix}
               a \\
               b \\
             \end{matrix}\right]
             = 
             \left[\begin{matrix}
               r \\
               0 \\
             \end{matrix}\right]
          \]
        <img style="background-color: white; padding: 10px"
             src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5e2c4e785a596b703c50d8a1fd3ff2f2990eb01c"><br/>
        <small>https://en.wikipedia.org/wiki/Givens_rotation</small>
        </section>

        <section data-transition="fade-out">
          <h2>Givens Rotation</h2>
          <pre data-id="code-animation"><code data-trim>
fvariable cos
fvariable sin

: srotg ( f: a b -- f: c s )
  fdup f0= if fdrop fdrop 1e 0e exit then
  fover fover fdup f* fswap fdup f* f+ fsqrt 1/f ( a b 1/h )
  frot fdup f0<= if fswap fnegate fswap then ( b ~1/h a )
  fover f* frot frot f* ;

: srot ( sx sy n -- )
  0 ?do over sf@ cos sf@ f* dup sf@ sin sf@ f* f+
        dup sf@ cos sf@ f* over sf@ sin sf@ f* f-
        dup sf! over sf! incy @ + swap incx @ + swap loop 2drop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Cblas</h2>
          <ul>
            <li>C wrapper around F77 BLAS</li>
            <li>Cleverly works around Row vs Column major order</li>
          </ul>
          <pre data-id="code-animation"><code data-trim>
 void cblas_saxpy(const int N, const float alpha,
                  const float *X, const int incX,
                  float *Y, const int incY);
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Row-Major vs Column-Major Order</h2>
          <ul>
            <li>FORTRAN = Column-Major Order</li>
            <li>C = Row-Major Order</li>
            <li>Forth = \o/ No arrays,<br/>I do what I want!</li>
            <li>Transpose parameters<br/>allow conversion</li>
          </ul>
          <img style="float: left"
               src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Row_and_column_major_order.svg/255px-Row_and_column_major_order.svg.png">
        </section>

        <section data-transition="fade-out">
          <h2>Cblas</h2>
          <pre data-id="code-animation"><code data-trim>
: sfcell/ ( n -- n ) 2/ 2/ ;

c-function cblas_dsdot cblas_dsdot n a n a n -- r
: dsdot ( sx sy n -- f: prod )
  -rot >r incx @ sfcell/ r> incy @ sfcell/ cblas_dsdot ;

c-function cblas_saxpy cblas_saxpy n r a n a n -- void
: saxpy ( sx sy n f: sa -- )
  -rot >r incx @ sfcell/ r> incy @ sfcell/ cblas_saxpy ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY / DAXPY - Level 1 Benchmark</h2>
          <ul>
            <li>1000x - Multiply and Adds of two 1M element vectors</li>
            <li>Try both single and double precision</li>
            <li>Compare Forth and BLAS</li>
            <li>Multiplies: 1 Billion</li>
            <li>Adds: 1 Billion</li>
            <li>BLAS Baseline: 0.454s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY / DAXPY - Level 1 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
1000000 constant test-size
1 sfloats incx ! 1 sfloats incy !

test-size sfloats allocate throw constant foo
foo test-size sfloats 33 fill
test-size sfloats allocate throw constant bar
bar test-size sfloats 33 fill
: benchmark
  1000 0 do
    123e foo bar test-size saxpy
  loop
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SAXPY_DAXPY.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>SGEMV - Level 2 Benchmark</h2>
          <ul>
            <li>10x - Vector-Matrix Multiply of size 10K x 10K</li>
            <li>Compare Forth and BLAS</li>
            <li>Compare Forth at Level 1+ vs Level 2 only</li>
            <li>Multiplies: 1.0002 Billion</li>
            <li>Adds: 1.0002 Billion</li>
            <li>BLAS Baseline: 1.200s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SGEMV - Level 2 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
10000 constant test-size
1 sfloats incx ! 1 sfloats incy !

test-size test-size * sfloats allocate throw constant mat
mat test-size test-size * sfloats 33 fill
test-size sfloats allocate throw constant vec
vec test-size sfloats 33 fill
test-size sfloats allocate throw constant vec2
vec2 test-size sfloats 33 fill
test-size sfloats lda !
: benchmark
  10 0 do
    9e 1e mat vec vec2 test-size test-size sgemv
  loop
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SGEMV.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>SGEMM - Level 3 Benchmark</h2>
          <ul>
            <li>One 1000 x 1000 Multiply and Add</li>
            <li>Compare Forth and BLAS</li>
            <li>Compare Forth at Level 1+, Level 2+, and Level 3 only</li>
            <li>Multiplies: 1.002 Billion</li>
            <li>Adds: 1.002 Billion</li>
            <li>BLAS Baseline: 0.462s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SGEMM - Level 3 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
1000 constant test-size
1 sfloats incx ! 1 sfloats incy !
#NoTrans transa !
#NoTrans transb !

test-size test-size * sfloats allocate throw constant mat1
mat1 test-size test-size * sfloats 33 fill
test-size sfloats lda !
test-size test-size * sfloats allocate throw constant mat2
mat2 test-size test-size * sfloats 33 fill
test-size sfloats ldb !
test-size test-size * sfloats allocate throw constant mat3
mat3 test-size test-size * sfloats 33 fill
test-size sfloats ldc !

: benchmark
  9e 1e mat1 mat2 mat3 test-size test-size test-size sgemm
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SGEMM.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>Logit</h2>
          \[ logit(x) = ln\left(\frac{x}{1 - x}\right) \space\space\space
             logit^{-1}(x) = \frac{1}{1 + e^{-x}} \]
          <img style="background-color: white" height="300"
               src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Logit.svg/525px-Logit.svg.png"><br/>
          <small>https://en.wikipedia.org/wiki/Logit</small>
        </section>

        <section data-transition="fade-out">
          <p>
          <br/>
          <a href="https://www.flagxor.com/">flagxor.com</a><br/>
          <a href="https://github.com/flagxor/svfig-talks">slides</a><br/>
          <a href="https://github.com/flagxor/svfig-talks/blob/gh-pages/svfig-2020-10-24/pi.fs">code</a><br/>
          </p>
          <h1>&#x2698;</h1>
          <h2>Thank you</h2>
        </section>
     </div>
    </div>

    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/math/math.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="../common/forth.js"></script>

  </body>
</html>
