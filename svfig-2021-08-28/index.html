<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Deep Learning in Forth - Part I</title>
    <meta name="description"
          content="Deep Learning in Forth - Part I - August 28, 2021">
    <meta name="author" content="Brad Nelson">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style"
    content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/league.css" id="theme">
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="../common/forth.css">
    <script src="../common/printing.js"></script>
    <script src="../node/node_modules/vis-network/standalone/umd/vis-network.min.js"></script>

<!--

Deep Learning in Forth - Part I

In recent years, multilayer artificial neural networks have made tremendous
strides solving a wide range of machine learning problems from image
recognition to the generation of random cats. While typical ML libraries
have hundreds of thousands of lines of code, Forth can be used to explore
this domain with far less complexity.

This talk will kick off a multi-part series erecting a lexicon for ML programs in Forth.

In Part I, we'll introduce key deep learning ideas including: convolutional
neural networks, automatic differentiation, and gradient descent.
We'll then build and benchmark low level building blocks for linear algebra
and format conversion that later parts in the series will use as a foundation.
-->

    <style>
pre {
  background-color: #202;
  padding: 10px;
}
.reveal-viewport {
  background: #1c0020;
  background: -moz-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -webkit-gradient(radial, center center, 0px, center center, 100%, color-stop(0%, #55005f), color-stop(100%, #1c0020));
  background: -webkit-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -o-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: -ms-radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background: radial-gradient(center, circle cover, #55005f 0%, #1c0020 100%);
  background-color: #2b002b;
}
.reveal h2 {
  color: #ff4;
}
.reveal h3 {
  color: #ffc;
}
.hljs-comment {
  color: #ff0;
}
.reveal a:link {
  color: #f0f;
}
.reveal a:visited {
  color: #f0f;
}
.reveal a:hover {
  color: #fff;
}
.reveal a:active {
  color: #fff;
}
.reveal .controls {
  color: #a0a;
}
.reveal .progress {
  color: #707;
}
  </style>
  </head>

  <body>
    <div style="position: fixed; bottom: 0; width: 100%">
      <center>
        <h2 style="font-family=monospace; color: blue" id="cc1"></h2>
        <h2 style="font-family=monospace" id="cc2"></h2>
      </center>
    </div>
    <div class="reveal">

      <div class="slides">
        <section data-transition="fade-out">
          <h2>Deep Learning in Forth<br/>Part I</h2>
          <h3>August 28, 2021</h3>
          <p>
          <small><a href="http://flagxor.com">Brad Nelson</a> /
            <a href="http://twitter.com/flagxor">@flagxor</a></small>
          </p>
        </section>

        <section data-transition="fade-out">
          <h2>Overview</h2>
          <ul>
            <li>Machine Learning is EXCITING!</li>
            <li>Why are neural networks "suddenly" working?</li>
            <li>Forth has a role to play!!!</li>
            <li>Introduce core Deep Learning</li>
            <li>Build a performant foundation in Forth</li>
            <li>More Next Time!</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Caveat</h2>
          <b>I am not an ML expert!</b>
        </section>

        <section data-transition="fade-out">
          <div>
          <img src="https://upload.wikimedia.org/wikipedia/commons/0/0b/Alphago_logo_Reversed.svg"
           style="background-color: white; padding: 5px" height="100"><br/>
          <small style="font-size: 10px">https://commons.wikimedia.org/wiki/File:Alphago_logo_Reversed.svg</small>
          </div> 
          <ul>
            <li>Monte-Carlo tree search guided by policy and value networks</li>
            <li>Bootstrapped from studying human games</li>
            <li>Quickly defeats human players
              <ul style="font-size: 26px">
                <li>Oct 2015 - 5:0 against Fan Hui (professional player)</li>
                <li>Mar 2016 - 4:1 against Lee Sedol</li>
                <li>May 2017 - 60:0 against professional including Ke Jie (#1 player)</li>
              </ul></li>
            <li>2017 - AlphaGo Zero, bootstraped from scratch, single computer with 4 TPUs</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>StyleGAN (2019)</h2>
          <img height="500" src="https://github.com/NVlabs/stylegan/raw/master/stylegan-teaser.png">
          <div class="tiny">
          https://github.com/NVlabs/stylegan/raw/master/stylegan-teaser.png</div>
        </section>

        <section data-transition="fade-out">
          <h2>DALE-E (2021)</h2>
          <div style="layout: inline-block">
          <small>"an armchair in the shape of an avocado." -----
                 "a snail made of a harp."</small>
          <img height="400" src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.37.37-pm.png">
          <span>-----</span>
          <img height="400" src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.39.12-pm.png">
          <div class="tiny">https://daleonai.com/dalle-5-mins</div>
        </section>

        <section data-transition="fade-out">
          <h2>Deep Nostalgia (2021)</h2>
          <iframe width="900" height="500" src="https://www.youtube-nocookie.com/embed/N12g3oDzOPo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </section>

        <section data-transition="fade-out">
          <h2>AI Dungeon - GPT-3 (2021)</h2>
<div style="text-align: left; font-size: 20px">
<p>
<small style="font-size: 10px">Generative Pre-trained Transformer 3</small><br/>
You are Gus the Destroyer, a knight living in the kingdom of Larion. You have a steel longsword and a wooden shield. You are on a quest to defeat the evil dragon of Larion. You've heard he lives up at the north of the kingdom. You set on the path to defeat him and walk into a dark forest. As you enter the forest you see a pale man. He is dressed in a white sleeveless shirt, brown pants, and boots. His head is completely hair and his face is expressionless. You draw your sword as the man stares at you.
</p>
<p>
&gt; You ask where the dragon is.<br/>
"Where is the dragon?" you ask.<br/>
"What?" the man asks as he begins to look a little more alert.<br/>
"I asked where the dragon is, I want to kill him."<br/>
</p>
<p>
&gt; You search the man.<br/>
You check the man for weapons but he is completely unarmed.<br/>
"Bah, you are new. Dragons are hard to kill, easier if you aren't a moron like you." The man sputters at you.
</p>
<p>
&gt; You head into the forest.<br/>
You set off towards the north of the forest. You walk slowly to the point where you are a good hour away from the pale man. A flock of small birds fly through the air and land on a nearby branch.
</p>
<p>
&gt; You look at birds.<br/>
You look down at the flock of birds. They are small birds and are chirping merrily away. They are in pretty bad shape.
</p>
</div>
        </section>

        <section data-transition="fade-out">
          <h2>Github Copilot - GPT-3 (2021)</h2>
          <img height="500" src="copilot.png">
          <div class="tiny">https://copilot.github.com/</div>
        </section>

        <section data-transition="fade-out">
          <h2>Why is this suddenly working?</h2>
          <ul>
            <li>Core idea from 1986 (Backpropagation), why now?</li>
            <li>LOTS more compute
               <ul>
                  <li>Moore's Law</li>
                  <li>GPUs &amp; TPUs</li>
                  <li>Cloud Computing</li>
               </ul></li>
            <li>LOTS more data
               <ul>
                  <li>Digital Cameras</li>
                  <li>Solid State Storage</li>
                  <li>The Internet</li>
               </ul></li>
            <li>Better techniques &amp; fixed early mistakes</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Where does Forth fit in?</h2>
          <ul>
            <li>Deep Learning involves powerful,<br/>but SIMPLE ideas</li>
            <li>Interactivity is important, not high-level performance (Python is the dominant language)</li>
            <li>Small ML models fit in embedded devices</li>
            <li>Big new things shift what's important</li>
          </ul>
          <img src="http://www.forth.org/svfig/svfig.jpg" height="200">
        </section>

        <section data-transition="fade-out">
          <h2>Deep Learning in a Nutshell</h2>
          <ul>
            <li>AI, Neurons, Aritificial Neurons, and Perceptrons</li>
            <li>Layers as Matrices (and Tensors)</li>
            <li>How "Deep" Learning Works</li>
            <li>Crucial details for making it work well</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Approaches to AI</h2>
          <ul>
            <li>Symbolic
              <ul style="font-size: 0.8em">
                <li>Popular initially</li>
                <li>Appealing because of Platonic ideal of knowledge</li>
                <li>Tractable because of limitations of early computers</li>
              </ul></li>
            <li>Connectionist
              <ul style="font-size: 0.8em">
                <li>Now very hot</li>
                <li>Appealing because it appears to mirror the brain</li>
                <li>Also appealing because it offers a model for "learning"</li>
                <li>Downside: "unexplainable" systems</li>
              </ul></li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Neurons</h2>
          <ul>
            <li>Firing frequency varies by stimulus</li>
            <li>Learning mechanism poorly understood</li>
          </ul>
          <img style="background-color: white; padding: 10px"
               src="https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png"><br/>
          <small>https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Neuron3.png</small>
        </section>

        <section data-transition="fade-out">
          <h2>Artificial Neurons</h2>
          <ul>
            <li>Mathematical / Engineering construct, inspired by biology</li>
            <li>Variations in layering and activation function</li>
          </ul>
          \[ y_j = A\left(\sum_{i=1}^n w_{i,j} x_i + b_j\right) \]
          x<sub>i</sub> = inputs<br/>
          w<sub>i,j</sub> = synapse weight, b<sub>j</sub> = bias<br/>
          A = activation function<br/>
        </section>

        <section data-transition="fade-out">
          <h2>Matrix Representation</h2>
          <ul>
            <li>Each layer is a matrix multiply with activation applied to each element</li>
          </ul>
          <img src="matrix.png"><br/>
        </section>

        <section data-transition="fade-out">
          <h2>Network Layers</h2>
          <ul>
            <li>Input/Ouput + zero or more "hidden" layers</li>
            <li>Multiple layers required for complex functions</li>
          </ul>
          <img src="layers.png"><br/>
        </section>

        <section data-transition="fade-out">
          <h2>Perceptron - 1958</h2>
          <ul>
            <li>Hardware Artificial Neural Network</li>
            <li>Single Layer (Linear Planes)</li>
          </ul>
          <img src="https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg"><br/>
          <small>https://en.wikipedia.org/wiki/Perceptron</small>
        </section>

        <section data-transition="fade-out">
          <h2>Improvements + Advances</h2>
          <ul>
            <li>Better Initialization</li>
            <li>Automatic Differentiation</li>
            <li>Better Activation Functions</li>
            <li>Better Layers</li>
            <li>Better Overall Structure</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Initialization</h2>
          <ul>
            <li>Biases - zero</li>
            <li>Random weights to balance variance between layers.</li>
          </ul>
<br/>
<br/>
\[
Glorot Uniform: Random(-limit, limit) \\
limit = \sqrt{\frac{6}{fan_{in} + fan_{out}}}
\]
        </section>

        <section data-transition="fade-out">
          <h2>How to Learn</h2>
          <ul>
            <li>Tune the network so that training input &rarr; output values produce the expected results</li>
            <li>Treat the whole network as a function <code>F</code>,
                and minimize <code>Loss(F(input), expected)</code></li>
            <li>Use a "loss function", like sum of squares or distance sum</li>
          </ul>
          <img src="hills.png" height="200">
        </section>

        <section data-transition="fade-out">
          <h2>How to Learn</h2>
          <ul>
            <li>Gradient Descent on weights and biases</li>
            <li>Move "downhill" in n-dimensions</li>
            <li>Need to be able to differentiate whole neural network + loss function</li>
          </ul>
          <img src="hills.png" height="200">
        </section>

        <section data-transition="fade-out">
          <h2>Ways to find a Derivative</h2>
          <ul>
            <li>Symbolic Differentiation</li>
            <li>Numeric Differentiation</li>
            <li>Automatic Differentiation</li>
          </ul>
          \[ \frac{\delta}{\delta x} ? \]
        </section>

        <section data-transition="fade-out">
          <h2>Automatic Differentiation</h2>
          <ul>
            <li>Compute the function for particular input</li>
            <li>Use intermediate results with chain rule to find derivative at that same input point</li>
            <li>Two approaches (for function for &#8477;<sup>n</sup> &rarr; &#8477;<sup>m</sup>)
              <ul>
                <li>Forward Accumulation (better if n &lt;&lt; m)</li>
                <li>Reverse Accumulation (better if n &gt;&gt; m)</li>
              </ul></li>
              <li>Minimizes round-off error</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Chain Rule</h2>
          <div>
          \[ \frac{\delta}{\delta x} f(g(x)) = \frac{\delta f}{\delta g} \frac{\delta g}{\delta x} \]
          </div>
        </section>

        <section data-transition="fade-out">
          <h2>Multivariable Chain Rule</h2>
          <div>
          \[ \frac{\delta}{\delta x} f(g_1(x), ..., g_k(x)) =
                 \sum_{i=1}^{k} \frac{\delta f}{\delta g_i} \frac{\delta g_i}{\delta x} \]
          </div>
        </section>

        <section data-transition="fade-out">
          <h2>Forward Accumulation</h2>
          <img src="https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png" height="500"><br/>
          <small>https://en.wikipedia.org/wiki/Automatic_differentiation#/media/File:ForwardAccumulationAutomaticDifferentiation.png</small>
        </section>

        <section data-transition="fade-out">
          <h2>Reverse Accumulation</h2>
          <img src="https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png" height="500"><br/>
          <small>https://en.wikipedia.org/wiki/Automatic_differentiation#/media/File:ReverseaccumulationAD.png</small>
        </section>

        <section data-transition="fade-out">
          <h2>AutoDiff with Matrices</h2>
          <ul>
            <li>Reverse Accumulation generalizes to matrices!</li>
            <li>Product rule works as you would expect</li>
            <li>Per-element operations also generalize</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Activation Functions</h2>
          <table>
          <tr>
            <td>\[ tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/180px-Activation_tanh.svg.png"></td>
          </tr>
          <tr>
            <td>\[ logistic(x) = \frac{1}{1 + e^{-x}} \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/180px-Activation_logistic.svg.png"></td>
          </tr>
          <tr>
            <td>\[ relu(x) = max(0, x) \]</td>
            <td><img style="background-color: white"
                     src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/180px-Activation_rectified_linear.svg.png"></td>
          </tr>
          </table>
          <small>https://en.wikipedia.org/wiki/Activation_function</small>
        </section>

        <section data-transition="fade-out">
          <h2>Convolutional Neural Networks</h2>
          <ul>
            <li>Image recognition of low level features should be the same throughout the visual field</li>
            <li>Why relearn it over and over?</li>
            <li>Apply the same windowed network throughout the image!</li>
          </ul>
          <img src="smiles.png" height="250">
        </section>

        <section data-transition="fade-out">
          <h2>Discrete Convolution</h2>
          \[
            (f * g)[n] = \sum_{m=-\infty}^{\infty} f[n - m] g[m]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>2D Discrete Convolution</h2>
          \[
            (f * g)[x, y] = \sum_{i=-\infty}^{\infty}
                            \sum_{j=-\infty}^{\infty} f[x - i, y - j] g[i, j]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>2D Discrete Convolution</h2>
          <table style="background-color: white; color: black; filter: grayscale(1)">
          <tr>
            <td>Identity</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png"></td>
          </tr>
          <tr>
            <td>Edge Detection</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f800ad5f76b6c26c729ff0c1fef44284d7cade7a"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/6/6d/Vd-Edge3.png"></td>
          </tr>
          <tr>
            <td>Gaussian Blur</td>
            <td><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ca9c0da52fe7818783942b06aac9cf396ae628bf"></td>
            <td><img src="https://upload.wikimedia.org/wikipedia/commons/2/28/Vd-Blur1.png"></td>
          </tr>
          </table>
          <small>https://en.wikipedia.org/wiki/Kernel_(image_processing)</small>
        </section>

        <section data-transition="fade-out">
          <h2>Convolutional Filters</h2>
          <img height="500" src="https://developer-blogs.nvidia.com/wp-content/uploads/2014/10/caffenet_learned_filters.png">
          <div class="tiny">
          https://developer.nvidia.com/blog/deep-learning-computer-vision-caffe-cudnn/</div>
        </section>

        <section data-transition="fade-out">
          <h2>Other Structures</h2>
          <ul>
            <li>Generative Adversarial Networks</li>
            <li>Recurrent Neural Networks</li>
            <li>Transformer Recurrent Neural Networks</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Pivot to Performance</h2>
          <ul>
            <li>We are doing LOTS of matrix operations!</li>
            <li>Matrix multiply will dominate, it's O(n<sup>3</sup>) when done naively</li>
            <li>Strassen's algorithm is O(n<sup>log<sub>2</sub>7</sup>) &#8776; O(n<sup>2.807</sup>)</li>
            <li>Best theoretical as of 2020 is O(n<sup>2.3728596</sup>), but has "galactic" constants</li>
            <li>How to best use the hardware?</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS</h2>
          <h3>Basic Linear Algebra Subprograms</h3>
          <ul>
            <li>FORTRAN Library</li>
            <li>Terse ≤6 character names</li>
            <li>Standard "Kernels"</li>
            <li>Strided Data</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Strided Data</h2>
          <img src="strides.png" height="550">
        </section>

        <section data-transition="fade-out">
          <h2>Type Convensions</h2>
          <code>
            <b style="color: #ff0">S</b>GEMM
            <b style="color: #ff0">D</b>GEMM
            <b style="color: #ff0">C</b>GEMM
            <b style="color: #ff0">Z</b>GEMM
          </code>
          <ul style="list-style-type: none">
            <li>S = Single precision (32-bit) floats</li>
            <li>D = Double precision (64-bit) floats</li>
            <li>C = Single precision complex (2 x 32-bit) floats</li>
            <li>Z = Double precision complex (2 x 64-bit) floats</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 1 (1979)</h2>
          <ul style="list-style-type: none">
            <li>*ROTG + SROT - Givens rotation</li>
            <li>*SWAP - Swap two vectors</li>
            <li>*SCAL - Vector scaling x = a*x</li>
            <li>*COPY - Vector copy</li>
            <li><b style="color: #ffc">*AXPY - Vector scale and add y = a*x + y</b></li>
            <li><b style="color: #ffc">*DOT - Dot product</b></li>
            <li>*NRM2 - Euclidean norm / vector length</li>
            <li>*ASUM - Vector sum of absolute values</li>
            <li>I*AMAX - Index of max absolute value</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 2 (1984-88)</h2>
          <ul style="list-style-type: none">
            <li>*GEMV - Matrix Vector Multiply
                \[ y := \alpha A x + \beta y \]</li>
            <li>*GER - Row-column Vector multiply
                \[ A := \alpha x y^T + A \]</li>
            <li>Matrix-vector operations on symmetric/triangular matrices
                stored in banded or packed format.</li>
            <li>Transpose generously supported.</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>BLAS - Level 3 (1990)</h2>
          <ul style="list-style-type: none">
            <li>*GEMM - Matrix Matrix Multiply
                \[ C := \alpha A B + \beta C \]</li>
            <li>Matrix-matrix operations on symmetric/triangular matrices
                stored in banded or packed format.</li>
            <li>Transpose generously supported.</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY</h2>
          <pre data-id="code-animation"><code data-trim>
       SUBROUTINE saxpy(N,SA,SX,INCX,SY,INCY)
       REAL SA
       INTEGER INCX,INCY,N
       REAL SX(*),SY(*)
       INTEGER I,IX,IY,M,MP1
       INTRINSIC mod
 
       IF (n.LE.0) RETURN
       IF (sa.EQ.0.0) RETURN
       IF (incx.EQ.1 .AND. incy.EQ.1) THEN
          m = mod(n,4)
          IF (m.NE.0) THEN
             DO i = 1,m
                sy(i) = sy(i) + sa*sx(i)
             END DO
          END IF
          IF (n.LT.4) RETURN
          mp1 = m + 1
          DO i = mp1,n,4
             sy(i) = sy(i) + sa*sx(i)
             sy(i+1) = sy(i+1) + sa*sx(i+1)
             sy(i+2) = sy(i+2) + sa*sx(i+2)
             sy(i+3) = sy(i+3) + sa*sx(i+3)
          END DO
       ELSE
          ix = 1
          iy = 1
          IF (incx.LT.0) ix = (-n+1)*incx + 1
          IF (incy.LT.0) iy = (-n+1)*incy + 1
          DO i = 1,n
           sy(iy) = sy(iy) + sa*sx(ix)
           ix = ix + incx
           iy = iy + incy
          END DO
       END IF
       RETURN
       END
					</code></pre>
          <small>http://www.netlib.org/lapack/explore-html/d8/daf/saxpy_8f_source.html</small>
        </section>

        <section data-transition="fade-out">
          <h2>Utilities</h2>
          <pre data-id="code-animation"><code data-trim>
1 sfloats constant sfloat
1 dfloats constant dfloat
variable incx   sfloat incx !
variable incy   sfloat incy !

: sf, ( r -- ) here sf! sfloat allot ;
: df, ( r -- ) here df! dfloat allot ;

: sxsy+ ( sx sy -- sx' sy' ) incy @ + swap incx @ + swap ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY</h2>
          <pre data-id="code-animation"><code data-trim>
: saxpy ( sx sy n f: sa -- )
  0 ?do over
    sf@ fover f* dup sf@ f+ dup sf! sxsy+
  loop 2drop fdrop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Basic Ops</h2>
          <pre data-id="code-animation"><code data-trim>
: sswap ( sx sy n -- )
   0 ?do dup sf@ over sf@ dup sf! over sf! sxsy+ loop 2drop ;

: sscal ( sx n f: sa -- )
  0 ?do dup sf@ fover f* dup sf! incx @ + loop drop fdrop ;

: scopy ( sx sy n -- )
  0 ?do over sf@ dup sf! sxsy+ loop 2drop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SDOT</h2>
          <pre data-id="code-animation"><code data-trim>
: sdsdot ( sx sy n f: sb -- f: prod )
  0 ?do over sf@ dup sf@ f* f+ sxsy+ loop 2drop ;
: sdot ( sx sy n -- f: prod ) 0e sdsdot ;
: dsdot ( sx sy n -- f: prod ) 0e sdsdot ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Summing and Searching</h2>
          <pre data-id="code-animation"><code data-trim>
: snrm2 ( sx n -- f: norm )
  0e 0 ?do dup sf@ fdup f* f+ incx @ + loop drop fsqrt ;

: sasum ( sx n -- f: abssum )
  0e 0 ?do dup sf@ fabs f+ incx @ + loop drop ;

: isamax ( sx n -- index )
  over sf@ fabs 0 -rot 0 ?do
    dup sf@ fabs fover f> if fdrop dup sf@ fabs nip i swap then
    incx @ +
  loop drop fdrop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Givens Rotation</h2>
          \[
             \left[\begin{matrix}
               c & -s \\
               s & c \\
             \end{matrix}\right]
             \left[\begin{matrix}
               a \\
               b \\
             \end{matrix}\right]
             = 
             \left[\begin{matrix}
               r \\
               0 \\
             \end{matrix}\right]
          \]
        <img style="background-color: white; padding: 10px"
             src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5e2c4e785a596b703c50d8a1fd3ff2f2990eb01c"><br/>
        <small>https://en.wikipedia.org/wiki/Givens_rotation</small>
        </section>

        <section data-transition="fade-out">
          <h2>Givens Rotation</h2>
          <pre data-id="code-animation"><code data-trim>
fvariable cos
fvariable sin

: srotg ( f: a b -- f: c s )
  fdup f0= if fdrop fdrop 1e 0e exit then
  fover fover fdup f* fswap fdup f* f+ fsqrt 1/f ( a b 1/h )
  frot fdup f0<= if fswap fnegate fswap then ( b ~1/h a )
  fover f* frot frot f* ;

: srot ( sx sy n -- )
  0 ?do over sf@ cos sf@ f* dup sf@ sin sf@ f* f+
        dup sf@ cos sf@ f* over sf@ sin sf@ f* f-
        dup sf! over sf! incy @ + swap incx @ + swap loop 2drop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Cblas</h2>
          <ul>
            <li>C wrapper around F77 BLAS</li>
            <li>Cleverly works around Row vs Column major order</li>
          </ul>
          <pre data-id="code-animation"><code data-trim>
 void cblas_saxpy(const int N, const float alpha,
                  const float *X, const int incX,
                  float *Y, const int incY);
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Row-Major vs Column-Major Order</h2>
          <ul>
            <li>FORTRAN = Column-Major Order</li>
            <li>C = Row-Major Order</li>
            <li>Forth = \o/ No arrays,<br/>I do what I want!</li>
            <li>Transpose parameters<br/>allow conversion</li>
          </ul>
          <img style="float: left"
               src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Row_and_column_major_order.svg/255px-Row_and_column_major_order.svg.png">
        </section>

        <section data-transition="fade-out">
          <h2>Cblas</h2>
          <pre data-id="code-animation"><code data-trim>
: sfcell/ ( n -- n ) 2/ 2/ ;

c-function cblas_dsdot cblas_dsdot n a n a n -- r
: dsdot ( sx sy n -- f: prod )
  -rot >r incx @ sfcell/ r> incy @ sfcell/ cblas_dsdot ;

c-function cblas_saxpy cblas_saxpy n r a n a n -- void
: saxpy ( sx sy n f: sa -- )
  -rot >r incx @ sfcell/ r> incy @ sfcell/ cblas_saxpy ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY / DAXPY - Level 1 Benchmark</h2>
          <ul>
            <li>1000x - Multiply and Adds of two 1M element vectors</li>
            <li>Try both single and double precision</li>
            <li>Compare Forth and BLAS</li>
            <li>Multiplies: 1 Billion</li>
            <li>Adds: 1 Billion</li>
            <li>BLAS Baseline: 0.454s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SAXPY / DAXPY - Level 1 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
1000000 constant test-size
1 sfloats incx ! 1 sfloats incy !

test-size sfloats allocate throw constant foo
foo test-size sfloats 33 fill
test-size sfloats allocate throw constant bar
bar test-size sfloats 33 fill
: benchmark
  1000 0 do
    123e foo bar test-size saxpy
  loop
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SAXPY_DAXPY.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>SGEMV - Level 2 Benchmark</h2>
          <ul>
            <li>10x - Vector-Matrix Multiply of size 10K x 10K</li>
            <li>Compare Forth and BLAS</li>
            <li>Compare Forth at Level 1+ vs Level 2 only</li>
            <li>Multiplies: 1.0002 Billion</li>
            <li>Adds: 1.0002 Billion</li>
            <li>BLAS Baseline: 1.200s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SGEMV - Level 2 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
10000 constant test-size
1 sfloats incx ! 1 sfloats incy !

test-size test-size * sfloats allocate throw constant mat
mat test-size test-size * sfloats 33 fill
test-size sfloats allocate throw constant vec
vec test-size sfloats 33 fill
test-size sfloats allocate throw constant vec2
vec2 test-size sfloats 33 fill
test-size sfloats lda !
: benchmark
  10 0 do
    9e 1e mat vec vec2 test-size test-size sgemv
  loop
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SGEMV.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>SGEMM - Level 3 Benchmark</h2>
          <ul>
            <li>One 1000 x 1000 Multiply and Add</li>
            <li>Compare Forth and BLAS</li>
            <li>Compare Forth at Level 1+, Level 2+, and Level 3 only</li>
            <li>Multiplies: 1.002 Billion</li>
            <li>Adds: 1.002 Billion</li>
            <li>BLAS Baseline: 0.462s</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>SGEMM - Level 3 Benchmark</h2>
          <pre data-id="code-animation"><code data-trim>
1000 constant test-size
1 sfloats incx ! 1 sfloats incy !
#NoTrans transa !
#NoTrans transb !

test-size test-size * sfloats allocate throw constant mat1
mat1 test-size test-size * sfloats 33 fill
test-size sfloats lda !
test-size test-size * sfloats allocate throw constant mat2
mat2 test-size test-size * sfloats 33 fill
test-size sfloats ldb !
test-size test-size * sfloats allocate throw constant mat3
mat3 test-size test-size * sfloats 33 fill
test-size sfloats ldc !

: benchmark
  9e 1e mat1 mat2 mat3 test-size test-size test-size sgemm
;
benchmark
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <img src="SGEMM.svg" height="700">
        </section>

        <section data-transition="fade-out">
          <h2>Intel MKL</h2>
          <ul>
            <li>Intel oneAPI Math Kernel Library</li>
            <li>Linux, Windows, and OSX Intel Optimized Math Kernels</li>
            <li>Structured to heavily focus on Intel over AMD</li>
            <li>Implements BLAS and other kernels like FFT</li>
            <li>Takes advantage of: threads, memory layout, SIMD, the works</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h3>Wall Clock - System BLAS vs MKL BLAS</h3>
          <img src="speedup.svg" height="550">
        </section>

        <section data-transition="fade-out">
          <h3>GigaFLOPS - System BLAS vs MKL BLAS</h3>
          <img src="gflops.svg" height="550">
        </section>

        <section data-transition="fade-out">
          <h3>Wall Clock - System BLAS vs MKL BLAS</h3>
          <img src="speedup2.svg" height="550">
        </section>

        <section data-transition="fade-out">
          <h3>GigaFLOPS - System BLAS vs MKL BLAS</h3>
          <img src="gflops2.svg" height="550">
        </section>

        <section data-transition="fade-out">
          <h2>Speed!</h2>
          <ul>
            <li>MKL SGEMM is 1,512x faster than high level Forth!</li>
            <li>But the layers were Matrix x Vector?</li>
          </ul>
        </section>

         <section data-transition="fade-out">
          <h2>Batches</h2>
          <img src="batches.png" height="450">
        </section>

        <section data-transition="fade-out">
          <h2>How to do fast convolution?</h2>
          <ul>
            <li>Using a custom kernel could work, but would require optimizing another core operation</li>
            <li>Can we transform our input so that matrix multiplication becomes convolution?</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>im2col</h2>
          \[
             F * I = im2col(F, F_s) \cdot im2col(I, F_s)
          \]
        </section>

        <section data-transition="fade-out">
          \[
             I = \left[\begin{matrix}
               1^* & 2^* & 3 & 4 \\
               5^* & 6^* & 7 & 8 \\
               9 & 10 & 11 & 12 \\
               13 & 14 & 15 & 16 \\
             \end{matrix}\right]
          \]
          \[
             im2col(I, [2, 2]) = \\
             \left[\begin{matrix}
               1^* & 2 & 3 & 5 & 6 & 7 & 9 & 10 & 11 \\
               2^* & 3 & 4 & 6 & 7 & 8 & 10 & 11 & 12 \\
               5^* & 6 & 7 & 9 & 10 & 11 & 13 & 14 & 15 \\
               6^* & 7 & 8 & 10 & 11 & 12 & 14 & 15 & 16 \\
             \end{matrix}\right]
          \]
        </section>

        <section data-transition="fade-out">
          <h2>im2col</h2>
          <pre data-id="code-animation"><code data-trim>
: im2col { src b w h c dst fw fh }
  w h * c * sfloats { sb }
  h c * sfloats { sw }
  c sfloats { sh }
  c fh * sfloats { shc }
  src dst
  b 0 do
    over swap
    w fw - 1+ 0 do
      over swap
      h fh - 1+ 0 do
        over swap
        fw 0 do
          2dup shc cmove
          shc + swap sw + swap
        loop
        nip
        swap sh + swap
      loop
      nip
      swap sw + swap
    loop
    nip
    swap sb + swap
  loop
  2drop
;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>im2col'</h2>
          <pre data-id="code-animation"><code data-trim>
: im2col-size { b w h c fw fh } b  w fw - 1+ *  h fh - 1+ *  c * ;
: im2col' { src b w h c dst fw fh }
  dst b w h c fw fh im2col-size 0 fill
  sfloat incx ! sfloat incy !
  w h * c * sfloats { sb }
  h c * sfloats { sw }
  c sfloats { sh }
  c fh * { shc# }
  shc# sfloats { shc }
  dst src
  b 0 do
    over swap
    w fw - 1+ 0 do
      over swap
      h fh - 1+ 0 do
        over swap
        fw 0 do
          2dup swap shc# 1e saxpy
          shc + swap sw + swap
        loop
        nip
        swap sh + swap
      loop
      nip
      swap sw + swap
    loop
    nip
    swap sb + swap
  loop
  2drop
;
				</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>im2col Performance</h2>
          <ul>
            <li>~1.8 sec for 1024 128x128x3 images</li>
            <li>~7.6 sec for multiply</li>
            <li>NOTE: The resulting matrix can be reshaped into expected output in place</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Tensors</h2>
          <ul>
            <li>Many multidimensional tensors can be flattened to big matrices and correctly multiplied after adjusting shape</li>
            <li>This will let us batch many items together</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>Multiple Filters and Images</h2>
          <img src="convolve.png" height="550">
        </section>

        <section data-transition="fade-out">
          <h2>First Deep Learning Target Project</h2>
          <ul>
            <li>Recognize handwritten digits</li>
            <li>Use a series of 3x3 convolution layers, followed by some dense layers</li>
            <li>Classify into 0-9</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>MNIST (1998)</h2>
          <ul>
            <li>Handwritten digit dataset</li>
            <li>28x28 grayscale</li>
            <li>60,000 training images</li>
            <li>10,000 test images</li>
            <li>Remixed of NIST high school student and census bureau datasets</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <h2>IDX File Format</h2>
          <pre>
Big Endian

$00    - Magic #                            (byte)
$00    - Magic #                            (byte)
type   - $08=uint8 $09=int8 $0B=int16       (byte)
         $0C=int32 $0D=float32 $0E=float64
rank   - Number of dimensions               (byte)
dim 1  - Size in dimension 1                (int32)
....                                        ....
dim N  - Size in dimension N                (int32)
data   - Raw data
          </pre>
        </section>

        <section data-transition="fade-out">
          <h2>Convert to Float</h2>
          <pre data-id="code-animation"><code data-trim>
: u8tof32 ( a a n -- )
  0 do
    over c@ s>f dup sf!
    1 sfloats + swap 1+ swap
  loop 2drop ;
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>Convert to Float</h2>
          <pre data-id="code-animation"><code data-trim>
\c void u8tof32(const uint8_t *src, float *dst, size_t n) {
\c   for (;n;--n) {
\c     *dst++ = *src++;
\c   }
\c }
c-function u8tof32 u8tof32 a a n -- void
					</code></pre>
        </section>

        <section data-transition="fade-out">
          <h2>MNIST (1998)</h2>
          <img height="500" src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/480px-MnistExamples.png">
          <div class="tiny">
          https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png</div>
        </section>

        <section data-transition="fade-out">
          <h2>Next Time</h2>
          <ul>
            <li>Build Neural Network Layer Words</li>
            <li>Implicitly build a reverse pass to calculate the gradient</li>
            <li>Current Code tally: ~1,292 lines</li>
          </ul>
        </section>

        <section data-transition="fade-out">
          <p>
          <br/>
          <a href="https://www.flagxor.com/">flagxor.com</a><br/>
          <a href="https://github.com/flagxor/svfig-talks">slides</a><br/>
          <a href="https://github.com/flagxor/svfig-talks/blob/gh-pages/svfig-2020-10-24/pi.fs">code</a><br/>
          </p>
          <h1>&#x2698;</h1>
          <h2>Thank you</h2>
        </section>
     </div>
    </div>

    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/math/math.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="../common/forth.js"></script>

  </body>
</html>
